I've been running weekend experiments with small open-source LLMs.

The goal: take research papers and industry claims, test them myself on local models, and see what actually happens vs what's published.

This week: the "Lost in the Middle" paper.

The paper found that large models ignore information buried in the middle of long contexts. Wanted to see if small open-source models (Gemma-2B, Gemma-4B, Llama-3B) behave the same way.

Built a test with 70-100 documents, varied where the answer was placed, and added distractors designed to confuse the model.

The small models showed a different pattern — they struggled more with information at the beginning, not the middle. Recency bias seems to dominate at this scale.

Practical takeaway: if you're building RAG with small local models, document ordering might matter differently than you'd expect.

Small experiment, not rigorous research — just trying to understand how these things actually behave versus what the papers say.

More weekend experiments coming.
